{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transport\n",
    "\n",
    "Note: Github's default LaTeX rendering messes with some of the equations here. Consider viewing the notebook on [NBViewer](https://nbviewer.jupyter.org/github/kheyer/OTDD/blob/main/Algorithms/Optimal%20Transport.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Optimal transport is a mathematical approach towards measuring the distance between two arbitrary distributions. We can think of the problem this way. Consider two sets of points:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kheyer/OTDD/main/media/distribution.png\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "We want to map each point in the source distribution to a corresponding point in the target distribution, such that the total distance between all coupled points is minimized. Note that in this framework, we cannot simply match each point in the source distribution to the closest point in the target distribution. We need to ensure that all target points are included in the mapping. For this particular distribution, the coupling would look like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kheyer/OTDD/main/media/distribution_coupled.png\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "How do we generate this coupling? First we calculate a cost matrix $C$. For this example, our cost is the squared euclidean distance between points. We then run an optimization routine to find the coupling matrix $\\pi_{OT}$ such that\n",
    "\n",
    "\\begin{align}\n",
    "\\pi_{\\text{OT}} ~=~ \\min_{\\pi} \\sum \\text{C}(x,y) \\pi(x,y)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kheyer/OTDD/main/media/cost_coupling.png\" height=\"80%\" width=\"80%\">\n",
    "\n",
    "We can extend this idea to continuous distributions by converting the sum to an integral. We solve the problem\n",
    "\n",
    "\\begin{align}\n",
    "\\pi_{\\text{OT}} ~=~ \\min_{\\pi} \\int \\text{C}(x,y) d\\pi(x,y)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kheyer/OTDD/main/media/gaussian.png\" height=\"80%\" width=\"80%\">\n",
    "\n",
    "## Formulas\n",
    "\n",
    "Now we'll give a more formal definition of this concept.\n",
    "\n",
    "We define our two samples as discrete measures in $\\mathbb{R}^{n}$ with locations $x_{1}, ..., x_{n} \\in \\chi_{x}$, $y_{1}, ..., y_{m} \\in \\chi_{y}$, along with weights $\\alpha \\in P(\\chi_{x})$, $\\beta \\in P(\\chi_{y})$.\n",
    "\n",
    "The weights $\\alpha$, $\\beta$ are positive values defining a probability distribution over the elements in $\\chi_{x}$, $\\chi_{y}$\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha ~=~ \\sum_{i=1}^{N}\\alpha_{i}\\delta_{x_{i}},\\quad \\beta ~=~ \\sum_{j=1}^{M}\\beta_{j}\\delta_{y_{j}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N}\\alpha_{i} ~=~ 1, \\quad \\sum_{j=1}^{M}\\beta_{i} ~=~ 1\n",
    "\\end{align}\n",
    "\n",
    "Next we define a cost function $c(x_{i}, y_{j})$ which defines the ground cost between $x_{i}$ and $y_{j}$. This can be any valid distance function. We can then construct our cost matrix $C_{i,j}$ which contains the pairwise costs between all points in $\\chi_{x}$ and $\\chi_{y}$.\n",
    "\n",
    "We can now define the optimal transport problem with the Kantorovich formulation:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{OT}(\\alpha, \\beta) ~=~ \\min_{x\\in\\Pi(\\alpha, \\beta)}\\int_{\\chi_{x} \\times \\chi_{y}} c(x,y)d\\pi(x,y)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\forall i,j, \\pi_{i,j} \\geq 0, \\sum_{j} \\pi_{i,j} ~=~ \\alpha_{i}, \\sum_{i} \\pi_{i,j} ~=~ \\beta_{j}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We can also define this in probabalistic terms:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{OT}(\\alpha, \\beta) ~=~ \\min_{(\\chi_{x}, \\chi_{y})} \\{ \\mathbb{E}_{\\chi_{x}, \\chi_{y}} c(\\chi_{x}, \\chi_{y}) : \\chi_{x} \\sim \\alpha, \\chi_{y} \\sim \\beta \\}\n",
    "\\end{align}\n",
    "\n",
    "If $\\chi$ is associated with some metric $d_{\\chi}$, we can formulat eour ground cost as $c(x,y) = d_{\\chi}(x,y)^{p}$ for $p \\geq 1$. Under this condition, we can define the p-Wasserstein as $\\text{W}_{p}(\\alpha, \\beta) = \\text{OT}(\\alpha, \\beta)^{1/p}$\n",
    "\n",
    "\n",
    "## Sinkhorn Divergence\n",
    "\n",
    "For computational optimal transport, we are dealing with finite samples $x_{1}, ..., x_{n}$, $y_{1}, ..., y_{m}$. Our cost matrix is then a $n \\times m$ matrix. \n",
    "\n",
    "In thise case, solving the optimal transport problem becomes a linear optimization program that scales cubically with sample size, often becoming computationally prohibative. We can make the problem more computationally tractable by adding entropy regularization. This allows us to calculate approximate transport plans much more easily.\n",
    "\n",
    "The problem becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{OT}_{\\epsilon}(\\alpha, \\beta) ~=~ \\min_{x\\in\\Pi(\\alpha, \\beta)}\\int_{\\chi_{x} \\times \\chi_{y}} c(x,y)d\\pi(x,y) + \\epsilon \\text{H}(\\pi | \\alpha \\otimes \\beta)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{H}(\\pi | \\alpha \\otimes \\beta) ~=~ \\int log(\\frac{d\\pi}{d\\alpha d\\beta})d\\pi\n",
    "\\end{align}\n",
    "\n",
    "Where $\\epsilon$ is a constant that determines the strength of the entropic regularization.\n",
    "\n",
    "Since we are dealing with discrete samples, we can also phrase these equations in terms of summations.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{OT}_{\\epsilon}(\\alpha, \\beta) ~=~ \\min_{\\pi_{i,j}} \\sum_{i,j} \\pi_{i,j} C_{i,j} + \\epsilon \\text{KL}(\\pi, \\alpha \\otimes \\beta)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{KL}(\\pi, \\alpha \\otimes \\beta) ~=~ \\sum_{i,j} [\\pi_{i,j} log \\frac{\\pi_{i,j}}{\\alpha_{i} \\beta_{j}} - \\pi_{i,j} + \\alpha_{i} \\beta_{j}]\n",
    "\\end{align}\n",
    "\n",
    "With entropic regularization, we can define the __Sinkhorn Divergence__.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{SD}_\\epsilon(\\alpha,\\beta)~=~ \\text{OT}_\\varepsilon(\\alpha,\\beta)\n",
    "~-~\\tfrac{1}{2}\\text{OT}_\\varepsilon(\\alpha,\\alpha)\n",
    "~-~\\tfrac{1}{2}\\text{OT}_\\varepsilon(\\beta,\\beta).\n",
    "\\end{align}\n",
    "\n",
    "The Sinkhorn Divergence allows us to calculate a fuzzy transport plan that is much more computationally tractable than the unregularized optimal transport problem. The regularization strength $\\epsilon$ allows us to control how fuzzy the transport plan is. Specifically, $\\epsilon$ allows Sinkhorn Divergences to interpolate between Optimal Transport and kernel norms.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{OT}_C(\\alpha,\\beta)\n",
    "~~ \\xleftarrow{0\\leftarrow \\epsilon} ~~\n",
    "\\text{S}_\\epsilon(\\alpha,\\beta)\n",
    "~~ \\xrightarrow{\\epsilon\\rightarrow +\\infty} ~~\n",
    "\\tfrac{1}{2}\\|\\alpha-\\beta\\|_{-C}^2.\n",
    "\\end{align}\n",
    "\n",
    "Small values of $\\epsilon$ give a more computationally expensive problem that is closer to Optimal Transport. Large $\\epsilon$ values give a computationally cheap problem that is closer to kernel norms.\n",
    "\n",
    "## Sinkhorn Algorithm\n",
    "\n",
    "How do we actually compute the transport plan? We can do this with the Sinkhorn algorithm. We start by decomposing \n",
    "\n",
    "\\begin{align}\n",
    "\\text{C}_{i,j} + \\epsilon ~ log \\frac{\\pi_{i,j}}{\\alpha_{i} \\beta_{j}} ~=~ f_{i} + g_{j}\n",
    "\\end{align}\n",
    "\n",
    "Which would give us the transport plan as \n",
    "\n",
    "\\begin{align}\n",
    "\\pi ~=~ \\exp\\frac{1}{\\epsilon}(f\\oplus g-C)\\,\\cdot\\,\\alpha\\otimes\\beta\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\pi ~=~ \\text{diag}(\\alpha_iU_i)\\,K_{i,j}\\,\\text{diag}(V_j\\beta_j)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "U_i ~=~ \\text{exp}(f_i / \\epsilon), \\quad V_j ~=~ \\text{exp}(g_j / \\epsilon), \\quad K_{i,j} ~=~ \\text{exp}(-\\text{C}_{i,j} / \\epsilon)\n",
    "\\end{align}\n",
    "\n",
    "With this setup, we can compute the Sinkhorn algorithm. The Sinkhorn algorithm iteratively updates\n",
    "\n",
    "\\begin{align}\n",
    "f_i \\leftarrow \\alpha_i \\oslash K_{i,j} g_j\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "g_j \\leftarrow \\beta_j \\oslash K_{i,j} f_i\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For numerical stability, we can express the vectors $f_i$, $g_j$ in log form as \n",
    "\n",
    "\\begin{align}\n",
    "f_i = \\epsilon ~ logU, \\quad g_j = \\epsilon ~ logV\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "U ~=~ \\frac{1}{(K(V\\beta))}, ~~~~ V ~=~ \\frac{1}{(K^T(U\\alpha))}.\n",
    "\\end{align}\n",
    "\n",
    "We can then define our Sinkhorn iterations as\n",
    "\n",
    "\\begin{align}\n",
    "g_j^{(n+1)}~&=~\n",
    "-\\varepsilon \\log K^T(U^{(n)}\\alpha)\\\\\n",
    "~&=~\n",
    "-\\varepsilon \\log \\sum_{i=1}^\\text{N} \\exp \\big( - \\varepsilon ~ c(x,y)+f_i^{(n)}/\\varepsilon +\\log\\alpha_i \\big)\\\\~\\\\\n",
    "f_i^{(n+1)}~&=~\n",
    "-\\varepsilon \\log K(V^{(n+1)}\\beta)\\\\\n",
    "~&=~\n",
    "-\\varepsilon \\log \\sum_{j=1}^\\text{M} \\exp \\big( -\\varepsilon~c(x,y)+g_j^{(n+1)}/\\varepsilon +\\log\\beta_i \\big)\n",
    "\\end{align}\n",
    "\n",
    "## Gaussian Approximation\n",
    "\n",
    "For large datasets, even the Sinkhorn algorithm is computationally infeasible. In this case, we can approximate the transport cost by assuming our measures $\\chi_{x}$, $\\chi_{y}$ are multivariate Gaussian distributions given by $\\chi_{x} \\sim N(\\mu_{x}, \\Sigma_{x})$. \n",
    "\n",
    "We can then calculate the 2-Wasserstein distance between our measures with the closed form solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{W}^2_{2}(\\alpha, \\beta) = \\| \\mu_{\\alpha} - \\mu_{\\beta} \\|^{2}_{2} + \\| \\Sigma^{1/2}_{\\alpha} - \\Sigma^{1/2}_{\\beta} \\|^{2}_{F}\n",
    "\\end{align}\n",
    "\n",
    "Sources:\n",
    "\n",
    "[Gradient Flows Between Sampled Measires](https://www.math.ens.fr/~feydy/Teaching/DataScience/gradient_flows.html)\n",
    "\n",
    "[Computational Optimal Transport](https://arxiv.org/pdf/1803.00567.pdf)\n",
    "\n",
    "[Geometric Dataset Distances via Optimal Transport](https://arxiv.org/pdf/2002.02923.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
